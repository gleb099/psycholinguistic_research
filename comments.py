# -*- coding: utf-8 -*-
"""Comments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ykn03w3ii5HLMPSnZgh7vdBI-0ARbtuv
"""

!pip install selenium

!pip install wordcloud

import re
import time
from tqdm import tqdm

import os
import googleapiclient.discovery

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from selenium import webdriver

import nltk
from nltk.corpus import stopwords as nltk_stopwords
from pymystem3 import Mystem
from wordcloud import WordCloud

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import *

"""# Парсинг комментариев с YouTube"""

DEVELOPER_KEY = "AIzaSyCBRtNymwdIk1yXpLE9BqlpNkgVGZyT4fE"
VIDEO_ID = "JjGEBCTmg2g"

# Функция для скачивания корневых комментариев
def youtube(nextPageToken=None):
    # Disable OAuthlib's HTTPS verification when running locally.
    # *DO NOT* leave this option enabled in production.
    os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"

    api_service_name = "youtube"
    api_version = "v3"

    youtube = googleapiclient.discovery.build(
        api_service_name, api_version, developerKey=DEVELOPER_KEY)

    request = youtube.commentThreads().list(
        part="id,snippet",
        maxResults=100,
        pageToken=nextPageToken,
        videoId=VIDEO_ID
    )
    response = request.execute()
    return response

  # Функция для скачивания реплаев на комментарии
def youtubechild(NextParentId, nextPageToken=None):
    # Disable OAuthlib's HTTPS verification when running locally.
    # *DO NOT* leave this option enabled in production.
    os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"

    api_service_name = "youtube"
    api_version = "v3"

    youtube = googleapiclient.discovery.build(
        api_service_name, api_version, developerKey=DEVELOPER_KEY)

    request = youtube.comments().list(
        part="id,snippet",
        maxResults=100,
        pageToken=nextPageToken,
        parentId=NextParentId
    )
    response = request.execute()
    return response

# Главная функция
def mainParseComments():
    # Скачиваем комментарии
    print('download comments')
    response = youtube()
    items = response.get("items")
    nextPageToken = response.get("nextPageToken")  # скачивается порциями, на каждую следующую выдаётся указатель
    i = 1
    while nextPageToken is not None:
        print(str(i * 100))  # показываем какая сотня комментариев сейчас скачивается
        response = youtube(nextPageToken)
        nextPageToken = response.get("nextPageToken")
        items = items + response.get("items")
        if i * 100 == 1000:
            break
        i += 1

    print(f"Final count: {len(items)}")  # Отображаем количество скачаных комментариев

    # Делаем датафрейм из полученной инфы
    comments = [line.get("snippet").get("topLevelComment").get('snippet').get('textOriginal') for line in items]
    comments_df = pd.DataFrame({'comment': comments})
    print(comments_df)
    comments_df.to_csv('youtuberesults.csv')

    print("done")

    return comments_df

def testGetDf():
    textMass = ["Коллеги, есть предложение от Российского общества знание. Приглашение принять участие в батле проекта Ученый говорит 9 марта в г. Махачкала Мероприятие Ученый говорит представляет собой батл между учеными по научным тематикам.", 
                "Как же все проходит? Вы подготавливаете 3 выступления по 15 минут с презентациями. В первом этапе, вас будет 4 ученых, каждый выступает по 15 минут и с помощью зрителей определяется, кто проходит в следующий раунд. И так 3 этапа, пока не останется 1 победитель.", 
                "Всего три раунда, которые проходят в один день. Количество слайдов и тематику вы выбираете сами) Нам важно чтобы вы получили удовольствие и чувствовали себя комфортно!",
                "Технические моменты: За 1 мероприятие вы получите гонорар. Площадка: Точка кипения (ул. Батырая, д.1) Время: с 14:30 до 17:30",
                "Логистику и проживание РО Знание полностью оплачивает.",
                "Если вас этот формат заинтересовал, напишите менеджеру. +7 910 492-23-98 Анастасия Попова.",
                "Мне слова батл и Махачкала в одном предложении как-то совсем не нравятся",
                "Прямо с языка сняли))",
                "А что? Там недалеко Каспийск и на приколе стоит Каспийский монстр (говорят...).",
                "Там есть другие города, если кто-то хочет выступать можно написать менеджеру она подскажет",
                "Даже есть Южно-Сахалинск",
                "Я монстра еще живого видел, т.е.в движении."
                ]

    textMass_df = pd.DataFrame({'comment': textMass})
    print(textMass_df)
    return textMass_df

def testAssosDf():
    str_ = "Прямо с языка сняли"

    assotiativeMass = list()
    assotiativeMass.append(str_)

    first = ["идти", "криво", "вперед", "сказать", "в глаза", "дорога", "смотреть", "говорить", "и налево", "по дороге"]
    second = ["ним", "вместе", "тобой", "в", "другом", "к", "кем"]
    third = ["длинный", "русский", "родной", "животных", "иностранный"]
    fourth = ["мерку", "лозунг", "снимали"]

    for i1 in first:
      for i2 in second:
        for i3 in third:
          for i4 in fourth:
            str_new = f"{i1} {i2} {i3} {i4}"
            assotiativeMass.append(str_new)
    
    assotiativeMass_df = pd.DataFrame({'comment': assotiativeMass})
    print(assotiativeMass_df)
    return assotiativeMass

comments_df = mainParseComments()

comments_df = testGetDf()

comments_mass_assos = testAssosDf()

comments_mass_assos

comments_df["comment"]

"""# Загрузка и обработка размеченного *датасета*"""

# Зададим путь к основной папке
path_main = r"C:\Users\gleb9\OneDrive\Рабочий стол\UNI\diploma\parsingYouTubeComments"

positive = pd.read_csv('/content/positive.csv',
                       sep=';',
                       header=None
                       )

negative = pd.read_csv('/content/negative.csv',
                       sep=';',
                       header=None
                       )

positive_text = pd.DataFrame(positive.iloc[:, 3])
negative_text = pd.DataFrame(negative.iloc[:, 3])

positive_text['label'] = [1] * positive_text.shape[0]
negative_text['label'] = [0] * negative_text.shape[0]

labeled_tweets = pd.concat([positive_text, negative_text])

labeled_tweets.index = range(labeled_tweets.shape[0])

labeled_tweets.columns = ['text', 'label']

labeled_tweets

"""# Очистка размеченного датасета"""

# Оставим в тексте только кириллические символы
def clear_text(text):
    clear_text = re.sub(r'[^А-яЁё]+', ' ', text).lower()
    return " ".join(clear_text.split())


# Напишем функцию удаляющую стоп-слова
def clean_stop_words(text, stopwords):
    text = [word for word in text.split() if word not in stopwords]
    return " ".join(text)

# Загрузим список стоп-слов
nltk.download('stopwords')
stopwords = set(nltk_stopwords.words('russian'))
np.array(stopwords)

# Протестируем работу функции очистки текста
text = labeled_tweets['text'][np.random.randint(labeled_tweets.shape[0])]
print(text)
print('=======================================')
print(clean_stop_words((clear_text(text)), stopwords))

start_clean = time.time()

labeled_tweets['text_clear'] = labeled_tweets['text'].apply(lambda x: clean_stop_words(clear_text(str(x)), stopwords))

print('Обработка текстов заняла: ' + str(round(time.time() - start_clean, 2)) + ' секунд')

labeled_tweets = labeled_tweets[['text_clear', 'label']]
labeled_tweets.to_csv(path_main + 'labeled_tweets_clean.csv')

"""# Получение TF-IDF векторных представлений размеченных текстов

"""

# Для обучения классификатора получим значения IDF (inference document frequency) для слов из тренировочного набора данных, 
# значения IDF равны логарифму отношения количества документов к количеству документов содержащих искомое слово. 
# Например для стандартных слов, которые встречаются практически в любом тексте IDF будет близок к единице, 
# а для специфичных, которые встречаются в одном тексте из 100 это значение будет равно уже 2 (если мы берем основание логарифма 10).
# Затем получив словарь со значениями IDF мы можем получить векторное представление каждого текста по следующему принципу, значения IDF слова умножаем на значения

labeled_tweets.columns = ['text', 'label']

# предварительно разделим выборку на тестовую и обучающую
train, test = train_test_split(labeled_tweets,
                               test_size=0.2,
                               random_state=12348,
                               )

print(train.shape)
print(test.shape)

test

# Сравним распределение целевого признака
for sample in [train, test]:
    print(sample[sample['label'] == 1].shape[0] / sample.shape[0])

"""# Предварительное обучение моделей"""

# Получим векторные представления текстов
count_idf_1 = TfidfVectorizer(ngram_range=(1, 1))

tf_idf_train_base_1 = count_idf_1.fit_transform(train['text'])
tf_idf_test_base_1 = count_idf_1.transform(test['text'])

print(tf_idf_test_base_1.shape)
print(tf_idf_train_base_1.shape)

sample = test.sample(n=1)['text']
sample_tf_idf = count_idf_1.transform(sample)

print(sample_tf_idf.shape)

array = sample_tf_idf.toarray()
print(array)

print(sample)

print(array[array != 0])  # конкретные слова, их значения idf умноженные на их количество встречаемости в этом тексте

"""# Логистическая регрессия"""

model_lr_base_1 = LogisticRegression(solver='lbfgs',
                                     random_state=12345,
                                     max_iter=10000,
                                     n_jobs=-1)

# """Получим прогноз и оценим качество модели"""

model_lr_base_1.fit(tf_idf_train_base_1, train['label'])

predict_lr_base_proba = model_lr_base_1.predict_proba(tf_idf_test_base_1)

# Два столбца, первые вероятность того, что текст отрицательный и вероятность того, что текст положительный
# Если пары сложить, то мы получим большой набор единиц

print(predict_lr_base_proba)
print()
print(predict_lr_base_proba.sum(axis=1))

# """В качестве сравнения сделаем классификатор который в качестве прогноза выдает случайное число в интервале от 0 до 1 """

def coin_classifier(X: np.array) -> np.array:
    predict = np.random.uniform(0.0, 1.0, X.shape[0])
    return predict

coin_predict = coin_classifier(tf_idf_test_base_1)

fif = plt.figure(figsize=(8, 6))

pd.Series(coin_predict) \
    .hist(bins=100,
          alpha=0.7,
          color='r',
          label='Coin'
          )

pd.Series(predict_lr_base_proba[:, 1]) \
    .hist(bins=100,
          alpha=0.7,
          color='b',
          label='TF-IDF LogisticRegression'
          )
plt.legend()
plt.show()

"""# Визуализация ROC-кривых классификаторов"""

fpr_base, tpr_base, _ = roc_curve(test['label'], predict_lr_base_proba[:, 1])
roc_auc_base = auc(fpr_base, tpr_base)

fpr_coin, tpr_coin, _ = roc_curve(test['label'], coin_predict)
roc_auc_coin = auc(fpr_base, tpr_base)

fig = make_subplots(1, 1,
                    subplot_titles=["Receiver operating characteristic"],
                    x_title="False Positive Rate",
                    y_title="True Positive Rate"
                    )

fig.add_trace(go.Scatter(
    x=fpr_base,
    y=tpr_base,
    fill = 'tozeroy',
    name="ROC base (area = %0.3f)" % roc_auc_base,
))

fig.add_trace(go.Scatter(
    x=fpr_coin,
    y=tpr_coin,
    mode='lines',
    line=dict(dash='dash'),
    name='Coin classifier (area = 0.5)'
))

fig.update_layout(
    height=600,
    width=800,
    xaxis_showgrid=False,
    xaxis_zeroline=False,
    template='plotly_dark',
    font_color='rgba(212, 210, 210, 1)'
)

"""# Матрицы ошибок"""

# Выведем матрицы ошибок
confusion_matrix(test['label'],
                 (predict_lr_base_proba[:, 1] > 0.5).astype('float'),
                 normalize='true',
                 )
# [[true-neg, false-neg(доля отрицательных, к-е квалифицировались как положительные)],отрицательные 
# [false-pos(доля положительных, к-е квалифицировались как отрицательные), true-pos]] положительные

"""# Визуализация важности признаков"""

# Получим веса признаков, то есть множители
# подобранные логистической регрессией
# для каждого компонента вектора tf-idf

weights = pd.DataFrame({'words': count_idf_1.get_feature_names_out(), # get_feature_names_out - веса из классификатора
                        'weights': model_lr_base_1.coef_.flatten()}) # coef_ - кожффициенты из лог регрессии
weights_min = weights.sort_values(by='weights')
weights_max = weights.sort_values(by='weights', ascending=False)

# сортировка по убыванию
weights_min = weights_min[:100]
weights_min['weights'] = weights_min['weights'] * -1
print(weights_min)

# сортировка по возрастанию
weights_max = weights_max[:100]
print(weights_max)

"""# Облако тегов"""

# Воспользуемся библиотекой wordcloud для генерации картинок

wordcloud_positive = WordCloud(background_color="white",
                               colormap='Blues',
                               max_words=200,
                               mask=None,
                               width=1600,
                               height=1600) \
                            .generate_from_frequencies(
                            dict(weights_max.values))

wordcloud_negative = WordCloud(background_color="black",
                               colormap='Reds',
                               max_words=200,
                               mask=None,
                               width=1600,
                               height=1600) \
                            .generate_from_frequencies(
                            dict(weights_min.values))

# Выведем картинки сгенерированные вордклаудом
fig, ax = plt.subplots(1, 2, figsize=(20, 12))

ax[0].imshow(wordcloud_positive, interpolation='bilinear')
ax[1].imshow(wordcloud_negative, interpolation='bilinear')

ax[0].set_title('Топ ' + \
                str(weights_max.shape[0]) + \
                ' слов\n с наибольшим положительным весом',
                fontsize=20
                )
ax[1].set_title('Топ ' + \
                str(weights_min.shape[0]) + \
                ' слов\n с наибольшим отрицательным весом',
                fontsize=20
                )

ax[0].axis("off")
ax[1].axis("off")

plt.show()

"""# Снижение размерности признакового пространства модели"""

fig = make_subplots(1, 1)

fig.add_trace(go.Histogram(
    x=weights.query('weights != 0')['weights'],
    # histnorm = 'probability',
    opacity=0.5,
    showlegend=False
))

fig.add_trace(go.Histogram(
    x=weights.query('weights > 0.25 or weights < -0.25')['weights'],
    # histnorm = 'probability',
    opacity=0.5,
    showlegend=False
))

fig.update_layout(
    height=600,
    width=800,
    xaxis_showgrid=False,
    xaxis_zeroline=False,
    template='plotly_dark',
    font_color='rgba(212, 210, 210, 1)'

)

# График, по к-ому видно, что большинство слов имеют вес довольно близкий к нулю
# очень маленькая часть слов имеет вес < 0.5 или > 0.5

# отсечем тексты, которые не особо влияют на ситуацию
# может качество не ухудшится, но хотя бы уменьим количество необходимых вычислений
vocab = weights.query('weights > 0.25 or weights < -0.25')['words']

print(vocab)

"""# Получим новые векторные представления текстов"""

count_idf = TfidfVectorizer(vocabulary=vocab,
                            ngram_range=(1, 1))

tf_idf_train = count_idf.fit_transform(train['text'])
tf_idf_test = count_idf.transform(test['text'])

print(tf_idf_test.shape)
print(tf_idf_train.shape)

# Заново обучаем логическую регрессию на обучающем наборе данных train
model_lr_base = LogisticRegression(solver='lbfgs',
                                   random_state=12345,
                                   max_iter=10000,
                                   n_jobs=-1)

model_lr_base.fit(tf_idf_train, train['label'])

"""# Получим прогноз и оценим качество модели"""

predict_lr_base_proba_1 = model_lr_base.predict_proba(tf_idf_test) # прогноз на тестовом наборе данных

# Оценим качество классификации
fpr_base_1, tpr_base_1, _ = roc_curve(test['label'], predict_lr_base_proba_1[:, 1])
roc_auc_base_1 = auc(fpr_base_1, tpr_base_1)

# Построим ROC-кривую но основе новых данных
fig = make_subplots(1, 1,
                    subplot_titles=["Receiver operating characteristic"],
                    x_title="False Positive Rate",
                    y_title="True Positive Rate"
                    )

fig.add_trace(go.Scatter(
    x=fpr_base,
    y=tpr_base,
    fill='tozeroy',
    name="ROC curve (area = %0.3f)" % roc_auc_base,
))

fig.add_trace(go.Scatter(
    x=fpr_base_1,
    y=tpr_base_1,
    fill='tozeroy',
    name="Less dimensity ROC curve (area = %0.3f)" % roc_auc_base_1,
))

fig.add_trace(go.Scatter(
    x=fpr_coin,
    y=tpr_coin,
    mode='lines',
    line=dict(dash='dash'),
    name='Coin classifier'
))

fig.update_layout(
    height=600,
    width=800,
    xaxis_showgrid=False,
    xaxis_zeroline=False,
    template='plotly_dark',
    font_color='rgba(212, 210, 210, 1)'
)

"""# Вывод"""

# Мы снизили размерность векторов tf-idf потеряв при этом 0.2% качества 
# (площадь под ROC кривой при размерности > 170К 0.808, 
# площадь под ROC кривой при размерности > 48K -- 0.806)
# Вполне адекватная цена за снижение размерности примерно в 3 раза

"""# Подбор оптимального порогового значения классификации"""

scores = {}

weight = 0.535 # небольшой перекос веса в сторону негативных комментариев
# но в то же время максимизируем сумму правильно угаданных негативных и позитивных

for threshold in np.linspace(0, 1, 100):
    matrix = confusion_matrix(test['label'],
                              (predict_lr_base_proba[:, 0] < threshold).astype('float'),
                              normalize='true',
                              )

    score = matrix[0, 0] * weight + matrix[1, 1] * (1 - weight)

    scores[threshold] = score

threshold_df = pd.DataFrame({'true_score': scores.values(),
              'threshold': scores.keys()},
             ).sort_values(by='true_score', ascending=False)

threshold_df

# list(threshold_df[['threshold']])
threshold = float(list(dict(threshold_df)['threshold'])[0])

# матрица ошибок с выбранным проговым значением
matrix = confusion_matrix(test['label'],
                          (predict_lr_base_proba[:, 0] < threshold).astype('int'),
                          normalize='true',
                          )

matrix

"""# Классификация не размеченных комментариев"""

# С учетом ассоциативного поля
sum_toxic = 0
count = 0
for item_7 in comments_mass_assos:

    comments_df["comment"][7] = item_7

    start_clean = time.time()

    comments_df['text_clear'] = comments_df['comment'].apply(lambda x: clean_stop_words(clear_text(str(x)), stopwords))

    print('Обработка текстов заняла: ' + str(round(time.time() - start_clean, 2)) + ' секунд')

    video_tf_idf = count_idf.transform(comments_df['text_clear'])
    video_negative_proba = model_lr_base.predict_proba(video_tf_idf)
    comments_df['negative_proba'] = video_negative_proba[:, 0]

    # print(comments_df["comment"].count())
    print()

    text_clear = comments_df["text_clear"][7]
    text = comments_df["comment"][7]

    print(text)

    tf_idf_text = count_idf.transform([text_clear])

    toxic_proba = model_lr_base.predict_proba(tf_idf_text)

    print('Вероятность негатива: ', toxic_proba[:, 0][0])
    sum_toxic += toxic_proba[:, 0][0]
    count += 1
    print()

count = 1150

print(sum_toxic/count)

# C помощью обученных tf-idf векторизатора и логистической регрессии получим оценки вероятности негатива в каждом из комментариев

# Очистим тексты комментариев под видео
start_clean = time.time()

comments_df['text_clear'] = comments_df['comment'].apply(lambda x: clean_stop_words(clear_text(str(x)), stopwords))

print('Обработка текстов заняла: ' + str(round(time.time() - start_clean, 2)) + ' секунд')

"""# Визуализация ключевых слов"""

video_counter = CountVectorizer(ngram_range=(1, 1))
video_count = video_counter.fit_transform(comments_df['text_clear'])

print(video_count.toarray().sum(axis=0).shape)

video_counter.get_feature_names_out().shape

# Сохраним списки Idf для каждого класса

video_frequence = pd.DataFrame(
    {'word': video_counter.get_feature_names_out(),
     'frequency': video_count.toarray().sum(axis=0)
     }).sort_values(by='frequency', ascending=False)

print(video_frequence.shape[0])

# Воспользуемся библиотекой wordcloud для генерации картинок

wordcloud_video = WordCloud(background_color="black",
                              colormap='Blues',
                              max_words=200,
                              mask=None,
                              width=1600,
                              height=1600) \
                            .generate_from_frequencies(
                            dict(video_frequence.values))

# Выведем картинки сгенерированные вордклаудом
fig, ax = plt.subplots(1, 1, figsize=(20, 12))

ax.imshow(wordcloud_video, interpolation='bilinear')

ax.set_title('Топ ' + \
                str(video_frequence.shape[0]) + \
                ' слов наиболее уникальных слов,\n ',
                fontsize=20
                )

ax.axis("off")
plt.show()

"""# Получение оценки негативности комментария"""

comments_df

# Получим оценки негатива для всех комментариев

video_tf_idf = count_idf.transform(comments_df['text_clear'])
video_negative_proba = model_lr_base.predict_proba(video_tf_idf)
comments_df['negative_proba'] = video_negative_proba[:, 0]

print(comments_df["comment"].count())
print()
sum_toxic = 0
count = 0
for i in range(comments_df["comment"].count()):
    # print(source)
    text_clear = comments_df["text_clear"][i]
    text = comments_df["comment"][i]

    print(text)

    tf_idf_text = count_idf.transform([text_clear])

    toxic_proba = model_lr_base.predict_proba(tf_idf_text)

    print('Вероятность негатива: ', toxic_proba[:, 0][0])
    sum_toxic += toxic_proba[:, 0][0]
    count += 1
    print()

print(sum_toxic / count)

# Выведем 5 случайных комментариев c оценкой негатива видео
for _ in range(5):
    source = comments_df.sample(n=1)
    # print(source)
    text_clear = source['text_clear'].values[0]
    text = source['comment'].values[0]

    print(text)

    tf_idf_text = count_idf.transform([text_clear])

    toxic_proba = model_lr_base.predict_proba(tf_idf_text)

    print('Вероятность негатива: ', toxic_proba[:, 0])
    print()

"""# Найдем доли негативных комментариев при оптимальном пороговом значении"""

# Выводим среднее значение
video_share_neg = (comments_df['negative_proba'] > threshold).sum() / comments_df.shape[0]

video_share_neg

"""# Гистограмма оценки комментариев под видео"""

fig = make_subplots(1, 1,
                    subplot_titles=['Распределение комментариев по оценке негативности']
                    )

fig.add_trace(go.Violin(
    x=comments_df['negative_proba'],
    meanline_visible=True,
    name='Video (N = %i)' % comments_df.shape[0],
    side='positive',
    spanmode='hard'
))

fig.add_annotation(x=0.8, y=1.5,
                    text=f"%0.2f — доля негативных комментариев (при p > {round(threshold, 2)})" % video_share_neg,
                    showarrow = False,
                    yshift = 10)

fig.update_traces(orientation='h',
                  width=1.5,
                  points=False
                  )

fig.update_layout(height=500,
                  # xaxis_showgrid=False,
                  xaxis_zeroline=False,
                  template='plotly_dark',
                  font_color='rgba(212, 210, 210, 1)',
                  legend=dict(
                      y=0.9,
                      x=-0.1,
                      yanchor='top',
                  ),
                  )
fig.update_yaxes(visible=False)

fig.show()